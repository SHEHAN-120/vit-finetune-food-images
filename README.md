# ğŸ› Vision Transformer (ViT) â€“ Indian Food Image Classification

This project fine-tunes a **Vision Transformer (ViT)** model (`google/vit-base-patch16-224-in21k`) to classify various **Indian food images** using the Hugging Face `transformers` and `datasets` libraries.

---

## ğŸ§  Project Overview

The goal of this project is to leverage transfer learning from a pre-trained Vision Transformer to recognize different Indian food dishes with high accuracy.
The dataset used is **[`rajistics/indian_food_images`](https://huggingface.co/datasets/rajistics/indian_food_images)**, available on the Hugging Face Hub.

---

## âš™ï¸ Tools and Frameworks Used

* ğŸ¥© **Hugging Face Transformers** â€“ for ViT model loading and fine-tuning
* ğŸ§  **PyTorch** â€“ backend deep learning framework
* ğŸ–¾ï¸ **Torchvision** â€“ for image transformations and preprocessing
* ğŸ“Š **Hugging Face Datasets** â€“ for dataset management
* ğŸ“ˆ **Evaluate library** â€“ for accuracy computation
* ğŸš€ **Google Colab** â€“ for GPU-based training and experimentation

---

## ğŸ” Preprocessing Techniques

| Step                    | Description                                                               |
| ----------------------- | ------------------------------------------------------------------------- |
| Image Loading           | Loaded images from Hugging Face dataset `rajistics/indian_food_images`.   |
| Resizing                | Used `RandomResizedCrop` to resize all images to 224Ã—224 pixels.          |
| Normalization           | Applied mean and std normalization from the pretrained ViT processor.     |
| Tensor Conversion       | Converted images to PyTorch tensors using `ToTensor()`.                   |
| Transformation Pipeline | Combined steps using `Compose([RandomResizedCrop, ToTensor, Normalize])`. |

---

## ğŸ§ª Model Architecture

* Base Model: `google/vit-base-patch16-224-in21k`
* Image size: 224 Ã— 224
* Patch size: 16 Ã— 16
* Classification Head: Replaced with a linear layer of size = number of classes in dataset
* Optimizer: AdamW (handled internally by Trainer)
* Evaluation Metric: Accuracy

---

## ğŸ—®ï¸ Training Configuration

```python
TrainingArguments(
    output_dir="train_dir",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=4,
    gradient_accumulation_steps=4,
    warmup_ratio=0.1,
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    report_to="none"
)
```

* The model was trained on **Google Colab (GPU)**.
* W&B logging was disabled using `report_to="none"` and `!pip uninstall -y wandb`.

---

## ğŸ’¾ Saving and Inference

After training, the model was saved locally:

```python
trainer.save_model("food_classification")
```

Then used for inference:

```python
from transformers import pipeline
pipe = pipeline("image-classification", model="food_classification")

from PIL import Image
import requests
from io import BytesIO

url = "https://marketplace.canva.com/fyIYY/MAGoWUfyIYY/1/tl/canva-overhead-shot-of-mushroom-pizza-MAGoWUfyIYY.jpg"
image = Image.open(BytesIO(requests.get(url).content))
pipe(image)
```

---

## ğŸ“Š Example Output

```
[{'label': 'pizza', 'score': 0.9987}]
```

---

## ğŸ Results Summary

| Metric   | Score                 |
| -------- | --------------------- |
| Accuracy | ~XX% (after 4 epochs) |

*(Replace XX% with your actual result once you evaluate.)*

---

---

## ğŸš€ How to Run

```bash
# Clone repo

# Install dependencies

# Run training script
```

---

## ğŸ§‘â€ğŸ’» Author

**Ravindu Shehan Induruwa**
Undergraduate at University of Ruhuna, Faculty of Science
ğŸ“§ mailto:ravindushehan[.ob@gmail.com](mailto:.ob@gmail.com)
ğŸŒ [LinkedIn](https://www.linkedin.com/in/shehan-induruwa-120abc) | [GitHub](https://github.com/SHEHAN-120)

---

## ğŸªª License

This project is licensed under the **MIT License** â€“ feel free to use or modify with attribution.

---
